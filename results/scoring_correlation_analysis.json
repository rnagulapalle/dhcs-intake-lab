{
  "scoring_table": [
    {
      "question_index": 1,
      "quality_score": 8.6,
      "passes_review": true,
      "llm_judge_score": 80,
      "ground_truth_score": 75.0
    },
    {
      "question_index": 2,
      "quality_score": 8.6,
      "passes_review": true,
      "llm_judge_score": 85,
      "ground_truth_score": 70.0
    },
    {
      "question_index": 3,
      "quality_score": 8.5,
      "passes_review": true,
      "llm_judge_score": 55,
      "ground_truth_score": 50.0
    },
    {
      "question_index": 4,
      "quality_score": 8.6,
      "passes_review": true,
      "llm_judge_score": 75,
      "ground_truth_score": 70.0
    },
    {
      "question_index": 5,
      "quality_score": 8.6,
      "passes_review": true,
      "llm_judge_score": 80,
      "ground_truth_score": 75.0
    },
    {
      "question_index": 6,
      "quality_score": 8.6,
      "passes_review": true,
      "llm_judge_score": 55,
      "ground_truth_score": 70.0
    },
    {
      "question_index": 7,
      "quality_score": 8.8,
      "passes_review": true,
      "llm_judge_score": 75,
      "ground_truth_score": 60.0
    },
    {
      "question_index": 8,
      "quality_score": 8.8,
      "passes_review": true,
      "llm_judge_score": 80,
      "ground_truth_score": 75.0
    },
    {
      "question_index": 9,
      "quality_score": 8.4,
      "passes_review": true,
      "llm_judge_score": 55,
      "ground_truth_score": 50.0
    },
    {
      "question_index": 10,
      "quality_score": 8.6,
      "passes_review": true,
      "llm_judge_score": 80,
      "ground_truth_score": 85.0
    }
  ],
  "distributions": {
    "quality_score_0_10": {
      "min": 8.4,
      "max": 8.8,
      "mean": 8.61,
      "median": 8.6,
      "stdev": 0.11972189997378667
    },
    "quality_score_scaled_0_100": {
      "min": 84.0,
      "max": 88.0,
      "mean": 86.1,
      "median": 86.0,
      "stdev": 1.1972189997378648
    },
    "llm_judge_score_0_100": {
      "min": 55,
      "max": 85,
      "mean": 72,
      "median": 77.5,
      "stdev": 12.064640713902572
    },
    "ground_truth_score_0_100": {
      "min": 50.0,
      "max": 85.0,
      "mean": 68.0,
      "median": 70.0,
      "stdev": 11.352924243950934
    },
    "calibration_gap": {
      "mean_gap": 14.1,
      "median_gap": 9.5,
      "interpretation": "Positive = internal score higher than judge"
    }
  },
  "correlations": {
    "quality_vs_llm_judge": {
      "correlation": 0.505,
      "n": 10,
      "mean_x": 86.1,
      "mean_y": 72,
      "std_x": 1.2,
      "std_y": 12.06
    },
    "llm_judge_vs_ground_truth": {
      "correlation": 0.65,
      "n": 10,
      "mean_x": 72,
      "mean_y": 68.0,
      "std_x": 12.06,
      "std_y": 11.35
    },
    "quality_vs_ground_truth": {
      "correlation": 0.383,
      "n": 10,
      "mean_x": 86.1,
      "mean_y": 68.0,
      "std_x": 1.2,
      "std_y": 11.35
    }
  }
}